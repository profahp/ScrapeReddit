{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\shawn\\anaconda3\\lib\\site-packages (1.0.5)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\shawn\\anaconda3\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\shawn\\anaconda3\\lib\\site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\shawn\\anaconda3\\lib\\site-packages (from pandas) (1.18.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shawn\\anaconda3\\lib\\site-packages (from python-dateutil>=2.6.1->pandas) (1.15.0)\n",
      "Requirement already satisfied: requests in c:\\users\\shawn\\anaconda3\\lib\\site-packages (2.27.1)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in c:\\users\\shawn\\anaconda3\\lib\\site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in c:\\users\\shawn\\anaconda3\\lib\\site-packages (from requests) (2.0.11)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\shawn\\anaconda3\\lib\\site-packages (from requests) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shawn\\anaconda3\\lib\\site-packages (from requests) (2021.5.30)\n",
      "Requirement already satisfied: pymongo in c:\\users\\shawn\\anaconda3\\lib\\site-packages (4.0.1)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\shawn\\anaconda3\\lib\\site-packages (1.25.9)\n",
      "Requirement already satisfied: datetime in c:\\users\\shawn\\anaconda3\\lib\\site-packages (4.4)\n",
      "Requirement already satisfied: pytz in c:\\users\\shawn\\anaconda3\\lib\\site-packages (from datetime) (2020.1)\n",
      "Requirement already satisfied: zope.interface in c:\\users\\shawn\\anaconda3\\lib\\site-packages (from datetime) (4.7.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shawn\\anaconda3\\lib\\site-packages (from zope.interface->datetime) (49.2.0.post20200714)\n"
     ]
    }
   ],
   "source": [
    "#----------Section1----------Install Packages-------------\n",
    "!pip install pandas\n",
    "!pip install requests\n",
    "!pip install pymongo\n",
    "!pip install urllib3\n",
    "!pip install datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'User-Agent': 'ADOAPI/0.0.1',\n",
       " 'Authorization': 'bearer 1525109076836-vgpHJ-g-NZRBHvRLM1kvSK977XnNIA'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#----------Section2----------authentication-------------\n",
    "CLIENT_ID = 'OJf9FMXFuoLCWQnOj0aaAw'\n",
    "SECRET_KEY = 'CKZApGhPDOFKjjrrzI6BbkiXoN5e5A'\n",
    "# username and password \n",
    "data = {\n",
    "    'grant_type': 'password', \n",
    "    'username': 'Heavy_Database3499', \n",
    "    'password': 'Wpi2022ADO'\n",
    "}\n",
    "#a short description of current API\n",
    "headers = {'User-Agent': 'ADOAPI/0.0.1'}\n",
    "\n",
    "import requests \n",
    "\n",
    "auth = requests.auth.HTTPBasicAuth(CLIENT_ID, SECRET_KEY)\n",
    "# access API, with auth, data, and headers\n",
    "res = requests.post('https://www.reddit.com/api/v1/access_token',\n",
    "                     auth=auth, data=data, headers=headers)\n",
    "TOKEN = res.json()['access_token']\n",
    "# format token to a string and put it into header\n",
    "headers['Authorization'] = f'bearer {TOKEN}'\n",
    "#display header\n",
    "headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------Section3----------Get a list of users you are intereated in-------------\n",
    "#use params to set the number of returned items\n",
    "sub = 'SustainableFashion'\n",
    "hotpost = 'hot'\n",
    "users = []\n",
    "res = requests.get('https://oauth.reddit.com/r/'+ subreddit +'/'+ hotpost, \n",
    "                  headers=headers, params={'limit': '5'})\n",
    "for post in res.json()['data']['children']:\n",
    "    users.append(post['data']['author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n"
     ]
    }
   ],
   "source": [
    "# extract id from a post \n",
    "# t3 kind means threads\n",
    "# post['kind']+'_'+post['data']['id']\n",
    "#use after to return specific data \n",
    "#res = requests.get('https://oauth.reddit.com/r/SustainableFashion/hot', \n",
    "#                  headers=headers, params={'limit':'100', 'after':'t3_th3ru0'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Export data to mongoDB\n",
    "#from pymongo import MongoClient\n",
    "#client = MongoClient()\n",
    "#client = MongoClient('localhost', 27017)\n",
    "#db = client.reddit\n",
    "#for post in res.json()['data']['children']:\n",
    "#    db.test.insert_one(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------Section4----------user profile scraper-------------\n",
    "import urllib, json\n",
    "import datetime as dt\n",
    "\n",
    "for name in users:\n",
    "        #two csv files per user, one for their comments, and one for their submissions\n",
    "    try:\n",
    "        comment_csv = open(name + '_comments.csv', 'r')\n",
    "        comment_csv.close()\n",
    "        continue\n",
    "    except IOError:\n",
    "        pass\n",
    "    comment_csv = open(name + '_comments.csv', 'w')\n",
    "\n",
    "    comment_csv.write(\"submission_id, parent_id, body, id, created, score \\n\")\n",
    "    #pull 1000 thread at a time\n",
    "    total = 1001\n",
    "    #return result before/after this date\n",
    "    after_field = \"0d\"\n",
    "    before_field = \"0d\"\n",
    "    while(total > 1000):\n",
    "        url = urllib.request.urlopen(\"https://api.pushshift.io/reddit/comment/search?subreddit=\" \n",
    "                                     +sub +\"&metadata=true&before=\" + before_field + \"&limit=1000&sort=desc&author=\" + name)\n",
    "        user_data = json.loads(url.read().decode())\n",
    "        comment_num = user_data[\"metadata\"][\"results_returned\"]\n",
    "        total = user_data[\"metadata\"][\"total_results\"]\n",
    "        for j in range(0, comment_num):\n",
    "            comment = user_data[\"data\"][j]\n",
    "            comment_csv.write('' .join([i if ord(i) < 128 else ' ' \n",
    "                                        for i in comment[\"link_id\"].replace(\",\", \"\").replace(\"\\n\", \".\")]) + \",\")\n",
    "            comment_csv.write('' .join([i if ord(i) < 128 else ' ' \n",
    "                                        for i in str(comment[\"parent_id\"]).replace(\",\", \"\").replace(\"\\n\", \".\")]) + \",\")\n",
    "            comment_csv.write('' .join([i if ord(i) < 128 else ' ' \n",
    "                                        for i in comment[\"body\"].replace(\",\", \"\").replace(\"\\n\", \".\")]) + \",\")\n",
    "            comment_csv.write('' .join([i if ord(i) < 128 else ' ' \n",
    "                                        for i in comment[\"id\"].replace(\",\", \"\").replace(\"\\n\", \".\")]) + \",\")\n",
    "            comment_csv.write(dt.datetime.fromtimestamp(comment[\"created_utc\"]).isoformat() + \",\")\n",
    "            comment_csv.write(str(comment[\"score\"]) + \"\\n\")\n",
    "            if(j == comment_num - 1):\n",
    "                before_field = str(user_data[\"data\"][comment_num - 1][\"created_utc\"])\n",
    "    comment_csv.close()\n",
    "\n",
    "\n",
    "    sub_csv = open(name + '_submissions.csv', 'w')\n",
    "    sub_csv.write(\"title, body, score, id, url, num_comments(approx.), created \\n\")\n",
    "    total = 1001\n",
    "    #return result before/after this date\n",
    "    after_field = \"0d\"\n",
    "    before_field = \"0d\"\n",
    "    while(total > 1000):\n",
    "        url = urllib.request.urlopen(\"https://api.pushshift.io/reddit/submission/search?subreddit=\" + sub + \"&metadata=true&before=\" + before_field + \"&limit=1000&sort=desc&author=\" + name)\n",
    "        user_data = json.loads(url.read().decode())\n",
    "        sub_num = user_data[\"metadata\"][\"results_returned\"]\n",
    "        total = user_data[\"metadata\"][\"total_results\"]\n",
    "        for j in range(0, sub_num):\n",
    "            submission = user_data[\"data\"][j]\n",
    "            sub_csv.write('' .join([i if ord(i) < 128 else ' ' for i in submission[\"title\"].replace(\",\", \"\").replace(\"\\n\", \".\")]) + \",\")\n",
    "            try:\n",
    "                sub_csv.write('' .join([i if ord(i) < 128 else ' ' for i in submission[\"selftext\"].replace(\",\", \"\").replace(\"\\n\", \".\")]) + \",\")\n",
    "            except KeyError:\n",
    "                sub_csv.write(\"[none],\")\n",
    "            sub_csv.write(str(submission[\"score\"]) + \",\")\n",
    "            sub_csv.write('' .join([i if ord(i) < 128 else ' ' for i in submission[\"id\"].replace(\",\", \"\").replace(\"\\n\", \".\")]) + \",\")\n",
    "            sub_csv.write('' .join([i if ord(i) < 128 else ' ' for i in submission[\"url\"].replace(\",\", \"\").replace(\"\\n\", \".\")]) + \",\")\n",
    "            sub_csv.write('' .join([i if ord(i) < 128 else ' ' for i in str(submission[\"num_comments\"]).replace(\",\", \"\").replace(\"\\n\", \".\")]) + \",\")\n",
    "            sub_csv.write(dt.datetime.fromtimestamp(submission[\"created_utc\"]).isoformat() + \"\\n\")\n",
    "            if(j == sub_num -1):\n",
    "                before_field = str(user_data[\"data\"][sub_num - 1][\"created_utc\"])\n",
    "    sub_csv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
